# TruthfulQA PHIâ€‘3.5Â Mini Fineâ€‘TuningÂ withÂ Unsloth

Fineâ€‘tuning Microsoftâ€™s **PHIâ€‘3.5Â Mini** language model on the **TruthfulQA** MCQs to curb hallucinationsâ€”especially for **multipleâ€‘choice question (MCQ) answering**.

> After fineâ€‘tuning, accuracy jumped from \~zero to **â‰ˆâ€¯91â€¯%** and response perplexity fell to **â‰ˆâ€¯1.78**.

---

## At a Glance

|                              | Before Fineâ€‘Tuning | After Fineâ€‘Tuning |
| ---------------------------- | ------------------ | ----------------- |
| **Accuracy**                 | 0.00 %                  | **90.85â€¯%**       |
| **Precision**                | 0.00 %                  | **90.90â€¯%**       |
| **Recall**                   | 0.00 %                  | **90.85â€¯%**       |
| **F1â€‘Score**                 | 0.00 %                  | **90.66â€¯%**       |
| **Avg. Responseâ€¯Perplexity** | 4.03                  | **1.78**          |
| **Avg. Corpusâ€¯Perplexity**   | 4.68                  | **2.27**          |

*(Evaluated on TruthfulQA MC test split; see the notebook for details.)*

the predictions before finetuning had some correct answers but very little compared to the whole validation data.

---

## ğŸ—‚ï¸ Repository Structure

```
truthfulQA_phi/
â”œâ”€â”€ final_phi_unsloth_finetuning.ipynb   <- **Working notebook (run this!)**
â”œâ”€â”€ *.ipynb                              <- Exploratory / hallucinated notebooks (kept for exploring different approaches)
â””â”€â”€ README.md                            <- You are here
```

---

## Quick Start (GoogleÂ Colab)

1. **Open** [`final_phi_unsloth_finetuning.ipynb`](final_phi_unsloth_finetuning.ipynb) in Colab (GPU runtime recommended).
2. **Run all cells**, **skipping** any cell that is commented out.
3. The notebook:

   1. Installs dependencies (Unsloth, Transformers, Bitsâ€‘andâ€‘Bytes, etc.).
   2. Downloads & preprocesses **TruthfulQA** into a tabular form with columns:

      ```text
      Type | Category | Question | BestÂ Answer | Options
      ```
   3. Applies prompt template:

      * **Alpacaâ€‘style PromptÂ (Modified)** â€“ delivers the best results (â‰ˆâ€¯91â€¯% accuracy)
   4. Fineâ€‘tunes PHIâ€‘3.5Â Mini via **LoRA** using **Unsloth**.
   5. Evaluates & prints metrics shown above.

---

## Environment & Dependencies

* **Pythonâ€¯3.10** (Colab default)
* **unsloth** â‰¥â€¯0.3.0
* **transformers** â‰¥â€¯4.41.0
* **datasets** â‰¥â€¯2.19.0
* **bitsandbytes**, **peft**, **accelerate**, **trl**, **torch** â‰¥â€¯2.3.0

> The install cell in the notebook does the required

---

## ğŸ” Dataset & Prompts

| Column        | Description                     |
| ------------- | ------------------------------- |
| `Type`        | Question type (e.g., Adversarial, non adversarial) |
| `Category`    | TruthfulQA topic domain (e.g., Misconceptions, Conspiracies)         |
| `Question`    | Naturalâ€‘language question       |
| `BestÂ Answer` | Groundâ€‘truth answer text        |
| `Options`     | semi-colonâ€‘separated MC options      |

Two prompt templates are experimented with:

1. **Î¦â€‘3.5 PromptÂ (Medium)** â€“ vanilla system / user messages (eval loss 0.78)
2. **Modified Alpaca Prompt** â€“ instructionâ€‘style with explicit "<|user|>", "<|end|>", and "<|assistant|>" sections â†’ **highest accuracy**. (eval loss 0.66)

---

## ğŸ“ˆ Results

| Metric                    | Value            |
| ------------------------- | ---------------- |
| Response Perplexity       | **1.779â€¯Â±â€¯0.01** |
| Accuracy                  | **90.85â€¯%**      |
| Precision                 | **90.90â€¯%**      |
| Recall                    | **90.85â€¯%**      |
| F1â€‘Score                  | **90.66â€¯%**      |
| Average Corpusâ€¯Perplexity | **2.27**         |

*See notebook for full evaluation script and confusion matrix.*

> **Note:** Base PHIâ€‘3.5â€‘Mini has \~3.8â€¯B parameters; LoRA adds \~1â€¯% trainable params.

---

## ğŸ“œ License & Attribution

### Code

Released under the **MIT License** â€“ see [`LICENSE`](LICENSE).

### Model

* **PHIâ€‘3.5Â Mini** Â© Microsoft, licensed under [MIT license](https://github.com/microsoft/phi-3-mini/blob/main/LICENSE) (check upstream for exact terms).
* **Unsloth** Â© Unsloth authors, Apacheâ€‘2.0.

### Dataset

* **TruthfulQA** Â© OpenAI, released under a CC BY 4.0 license.

> Respect each upstream license when distributing checkpoints.

---

## Preprint

A detailed writeâ€‘up is available:

* **arXiv:** [https://arxiv.org/abs/2501.01588](https://arxiv.org/abs/2501.01588)
* **Qeios:** [https://www.qeios.com/read/Z95X8O](https://www.qeios.com/read/Z95X8O)

*(Draft in progress; links will be updated when the final version is published.)*

---

## Acknowledgements

* Microsoft & the PHI team for openâ€‘sourcing PHIâ€‘3.5Â Mini.
* The **Unsloth** developers for streamlined fineâ€‘tuning.
* The creators of **TruthfulQA** for the benchmark.
* Google Colab for free GPU resources.

---

## Citation

If you use this work, please cite the preprint (see above) **and** the original TruthfulQA & PHI papers.

```
@misc{truthfulqa_phi,
  title        = {(WHY-PHI) Fine-Tuning PHI-3 for Multiple-Choice Question Answering: Methodology, Results, and Challenges},
  author       = {Hisham, Mohamed},
  howpublished = {GitHub},
  year         = {2025},
  note         = {\url{https://github.com/MohamedHisham20/truthfulQA_phi}}
}
```

---

## Contributing

Issues and pull requests are welcome! Please open an issue to discuss major changes first.

---

## Contact

MohamedÂ Hisham â€¢ [mohamed.hisham.abdellatif03@gmail.com](mailto:mohamed.hisham.abdellatif03@gmail.com)
